%\chapter{Background}
\chapter{Background}

In this chapter, we will provide the background needed for this thesis.

\section{Distributed Systems}\label{background:distributed_systems}
In the start of their book, Steen and Tanenbaum\cite{steen_distributed_2017} defines distributed systems loosely: 
\say{A distributed system is a collection of autonomous computing elements that appears to its users as a single coherent system.}

This definition describes two essential characteristics of distributed systems. Firstly, we can think of a distributed system as several computers or programs that work independently. They share workload and storage between them, but they are independent in the same sense as processes. The different programs or computers are referred to as \textit{nodes}. Each node in the system will work concurrently. Concurrency is one of the great benefits of a distributed system because it can distribute heavy computational tasks. One computer can have several nodes, but usually, we think of different computers as different nodes. The other characteristic the definition above describes is that the users should only see a single coherent system. This means that the user should not notice that the application runs on a distributed system.


Distributed systems have no shared clock. This is because having a shared clock is almost impossible, as even atomic clocks are not accurate enough. As a consequence, the system has to utilize other methods to ensure synchronization. We will discuss synchronization in section \ref{cascading_search}. Each node will have an independent notion of time, and the system as a whole will not have a global clock\cite{steen_distributed_2017}. This is also important because even though the nodes work with each other, they should always be loosely coupled and as independent as possible.


The nodes will exchange information, but they do not have shared memory to ensure \textit{loose coupling}, which means that nodes are not too dependent on each other. In some systems, it is vital with loose coupling because the nodes in the system can be heterogeneous. Two nodes can have quite different environments, such as different Operating System(OS), processing power, network, memory, and hardware, but still work together in a distributed system. Having a loosely coupled system will ensure that the system is robust and that it does not need specific hardware to run.

A distributed system is dynamic, which means that nodes may leave or join the system at any time. This is useful for scaling, as one can easily add nodes on demand. However, it also raises the issue that a node might disappear while doing a task.



\subsection{Distribution Transparency}
Making distributed system seem like one single system, is called \textit{distribution transparency}. A user should not know that it is interacting with a distributed system. Steen and Tanenbaum\cite{steen_distributed_2017} brings up several types of transparency that helps achieve distribution transparency. Following, is a brief description of them.

\subsubsection{Access Transparency}
Access transparency means hiding differences in machine architectures. The nodes need to agree on how to represent data. This helps the application use local and remote resources as the same. 

\subsubsection{Location Transparency}
Location transparency is about not letting the user tell where an object is physically located in the system. It should seemingly be local but might be on a different computer.

\subsubsection{Relocation Transparency}
Relocation transparency means hiding when an object is moved from one node to another while it is in use. The application should still be usable, and the user should not notice that the object has moved.

\subsubsection{Migration Transparency}
Migration transparency is offering the mobility of resources to the users. The users can change the location of objects without it disrupting the program. 

In other words, relocation transparency means hiding movement of objects, while migration transparency means letting the users move the objects.

\subsubsection{Replication Transparency}
Replication transparency is about hiding the process of an object being replicated. It can be hard to keep all replicas up to date, but the users should not know which replica it is accessing.

\subsubsection{Concurrency Transparency}
Concurrency transparency means not letting users know that they are accessing the same objects simultaneously. For example, if several users are accessing the same database, they should not notice that other users are also querying the same database.

\subsubsection{Failure Transparency}
Failure transparency means hiding an object or node failure from a user. One should not assume that a node will never crash or become unreachable. The system should be able to recover and handle failures without the user noticing.


\subsection{Advantages}
Some of the advantages of distributed systems are scalability, reliability, fault tolerance, and increased performance. Each of these characteristics is on its own very good to have, but the sum of them is what makes distributed systems excellent.

\subsubsection{Scalability}
Scalability means being able to scale up to meet demand. As more and more computation and storage move to the cloud, this is very important. Scalability can be measured in three different dimensions, namely \textit{Size, Geographical and Administrative scalability}.

\textbf{Size scalability} means being able to add more resources and clients to the system without loss of performance \cite{steen_distributed_2017}. Scaling is challenging as many factors can act as a bottleneck. For instance, it does not matter how big the data center is if the bandwidth is too small to handle all the users. Another example is that the hardware might not be fast enough. To ensure that the system can handle a continuously increasing amount of users and requests, these bottlenecks have to be handled. Most of the problems do come from a lack of resources on the servers. To fix this, one can upgrade the hardware, which is referred to as scaling up. A different option is to deploy more servers, which is referred to as scaling out.

\textbf{Geographical Scalability} denotes having nodes (most likely servers) that are far from the client, but the delay from communicating with them is hardly noticed. In a wide-area system, we might have nodes on different continents. The delay of sending messages between them is then too high not to be noticed in some situations. Also, because the packets are traveling through many switches and routers, it is much less reliable. Some stretches might have limited bandwidth, which can be an additional bottleneck for the system. To mitigate latency issues, the system can try to minimize waiting for a result. The system can do this by using asynchronous communication, which means that it can avoid waiting for a result from others as much as possible.

\textbf{Administrative Scalability} means scaling a distributed system across several independent administrative domains \cite{steen_distributed_2017}. This can be hard, as the domains can use different rules and conventions for communication, payment, resource usage, read-write privileges, et cetera. There is also a security aspect of it, as the domains might have different, and possibly conflicting, views on security.


\subsubsection{Fault Tolerance}
A distributed system should be fault-tolerant. When something fails, the system should handle it without the user or client knowing. Since it is often spread over several nodes, we can easily avoid having a single point of failure. Therefore it is called a \textit{partial failure}\cite{steen_distributed_2017} when something fails in a distributed system. Fault tolerance is vital to help to keep the system reliable and maintainable. 

\subsubsection{Performance}
There is little reason to use distributed systems if you do not get a performance boost while using it. Of course there are other reasons, like fault tolerance or convenience, but performance is at the heart of distributed systems. The fact that you can distribute the workload across several machines will in most cases give you a significant speed-up. Having several machines able to receive requests lets you have tremendous pressure from millions of requests without problems.
To split the workload we need multiprocessing. Multiprocessing lets us divide different work to different machines. For example, it can be the job of one machine to receive requests and forward them to a set of machines who do the actual work. Essentially, it enables doing things in parallel, which is what gives us speed-up in the first place.

\subsection{Pitfalls}
Steen and Tanenbaum \cite{steen_distributed_2017} bring up several pitfalls that one should avoid when making a distributed system. One should avoid assuming that:
\begin{itemize}
    \item The network is reliable
    \item The network is secure
    \item The network is homogeneous
    \item The topology does not change
    \item Latency is zero
    \item Bandwidth is infinite
    \item Transport cost is zero
    \item There is one administrator
\end{itemize}
All of these are important to have in mind while creating a distributed system. However, we will mostly focus on the pitfalls \textit{The network is homogeneous, The topology does not change, Latency is zero, Bandwidth is infinite}.

\textbf{The network is homogeneous} \\
In IoT we are dealing with a very heterogeneous network. Most of the devices do not use the same architecture or hardware.

\textbf{The topology does not change} \\
This is not true at all in the era of IoT. There is tons of mobile devices, like cellphones, that move around all the time, and changes the topology of the network. When going any further out than a LAN, we are also using hardware that we do not have control over. This means that they can move or disappear at any time.

\textbf{Latency is zero} \\
In a wide-area distributed system, the latency can be pretty high. If zero latency is assumed, we can run into problems that will affect the client quite a bit. Since the results can take some time to come back, we get significant waiting time, which is not considered. The user is then affected by this, as it has to wait for the client. 

\textbf{Bandwidth is infinite} \\
Some applications require substantial bandwidth. For example, a streaming platform might need bandwidth to stream high-quality video. However, the bandwidth varies everywhere.


\subsection{Middleware}
To ease the development process in distributed systems, we need \textit{middleware}. Middleware is a layer running on top of the OS of each machine in the system. It connects all the applications running on top of it. It provides an interface to the application, which helps the application with resource management, communication with other nodes, security, and recovery\cite{steen_distributed_2017}. An example of middleware is the Emerald VM, which we will discuss in the Emerald section \ref{Emerald}. 


\subsection{Types of Distributed Systems}
% on example of a cluster computing system is PlanetLab, which we will talk about later.
Distributed systems can generally be divided into two types: \textit{Cluster computing} and \textit{Grid computing}.

\subsubsection{Cluster computing}
Cluster computing is a collection of computers that usually has homogeneous architecture and hardware and resides in about the same place geographically. These computers usually run the same OS and effectively distribute (and therefore parallelize) heavy computational tasks. They can easily communicate because of the low latency and high reliability of LANs. Traditional data centers are an example of Cluster computing. 

\subsubsection{Grid computing}
Grid computing consists of several nodes spread across different domains and usually not geographically close. In Grid computing, there is more heterogeneity. Each node might have different hardware and different OS. Grid computing networks are often brought together in a virtual organization. An example of this is PlanetLab, which we will discuss later. 




% ---------------------------------------------------------------




\section{Cloud Computing}
There are several definitions of cloud computing. Many of them are similar, but one that sums them up is this one defined by NIST\cite{mell_nist_nodate}:
\say{Cloud computing is a model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction…}.


\begin{figure}[t]
    \centering
    \includegraphics[scale=0.7]{chapters/2_background/figures/Simplified_cloud.png}
    \caption{A simplified diagram of Cloud Computing}
    \label{fig:SimplifiedCloudDiagram}
\end{figure}

At a very high level, Cloud Computing is where devices offload their work to the usually distant cloud. As shown in figure~\ref{fig:SimplifiedCloudDiagram}, a laptop or any other device can communicate with the cloud, and let it provide storage and computation. The Cloud is a distributed system usually built as a Grid Computing system in a data center. When working with the cloud, we typically have thin clients and thick clients. For traditional cloud computing, the thin client is, for example, a laptop or cell phone, while the thick client is the server or servers in the distant data center. 

In today's society cloud computing has become much more advanced than traditional Cloud Computing. There is an increasing number of cloud service providers that offer Infrastructure as a Service(IaaS), Platform as a Service(PaaS) \textit{and} Software as a Service(SaaS), instead of just IaaS, like Amazon Web Services(AWS), Digital Ocean, Alibaba Cloud, Microsoft Azure and Google Cloud Platform(GCP), just to name a few. The biggest of them all is AWS, which holds almost half of the market share\cite{noauthor_cloud_2019}. The Cloud market made 129 billion dollars in 2020 alone\cite{noauthor_cloud_nodate}.

Even though the largest cloud providers have established themselves in many places, they still might not be geographically close enough to the client. Latency-aware applications need to minimize the physical distance between the client and the server. Therefore, latency-aware applications can use local cloud providers or firms like Akamai, which specializes in having servers geographically close to, and as few hops as possible away from the end-users.





% ----------------------------------------------





\section{Fog Computing}

Vaquero and Rodero-Merino\cite{vaquero_finding_2014}, define fog computing like this: 
\say{Fog computing is a scenario where a huge number of heterogeneous (wireless and sometimes autonomous) ubiquitous and decentralised devices communicate and potentially cooperate among them and with the network to perform storage and processing tasks without the intervention of third parties. These tasks can be for supporting basic network functions or new services and applications that run in a sandboxed environment. Users leasing part of their devices to host these services get incentives for doing so.}

As in other fields, there is no de-facto definition and the above definition is debatable. Since Fog Computing is a relatively new concept, different definitions will be used depending on the field they are in.

Fog computing is an extension of the Cloud Computing paradigm. It consists of nodes, like servers or desktops, that devices can offload the computation and storage to. The difference is that in Fog computing the fog nodes are located in close proximity to the user\cite{msftadmin_concept_2020}, as shown in figure~\ref{fig:FogDiagram}. The distant cloud has way more resources than the nodes in the fog layer, but since the fog nodes are closer than the distant cloud, they are great at doing latency-sensitive work. The fog nodes are usually defined as being on the same LAN, but some argue that they can be placed further away, at least geographically speaking. 

In Fog computing one would like as few jumps as possible to the node. A jump is when a packet moves from one network node to another, for example from a cellphone to an Access Point. If the nodes are on the same LAN, the number of jumps is predictable and low.

For instance, a Traceroute\cite{noauthor_traceroute68_nodate} from Oslo, Norway to a PlanetLab server in Rostock, Germany had at least 11 hops. With each hop adding latency because of processing, it’s easy to conclude that it is beneficial to keep hops at a minimum.

\begin{figure}[t]
    \centering
    \includegraphics[scale=0.6]{chapters/2_background/figures/Fog.png}
    \caption{Diagram of Fog Computing}
    \label{fig:FogDiagram}
\end{figure}

\subsection{Cisco}
Cisco\cite{bonomi_fog_nodate} and Bonovi was the first to coin the term \say{fog computing}, and defines it as \say{...a highly virtualized platform that provides compute, storage, and networking services between end devices and traditional Cloud Computing Data Centers, typically, but not exclusively located at the edge of network.}.  The characteristics of Fog Computing is that it is located at the edge, has location awareness, low latency, is easily geographically distributed, heterogeneity, support for mobility and some others. All these as a sum help provide a service that is needed in several modern applications, for example Augmented Reality and in general with IoT.

A Fog node is located at the edge in the sense that it is close to the end user or client. Because of this, the Fog node will have \textit{location awareness} and \textit{low latency}. Since they are not big servers, the Fog nodes are also easily \textit{geographically distributed}, and should be easy to install and maintain. Most likely the Fog nodes do not have the same hardware or software, which gives us the \textit{heterogeneity} characteristic. 

\subsection{A Survey of Fog Computing} % TODO: REWRITE this
The goal of Fog Computing is to improve Quality of service(QoS) with computation and storage offloading. Yi, Li, and Li, in ‘A Survey of Fog Computing’\cite{yi_survey_2015} argues that QoS can be divided into four aspects: \textit{Connectivity, reliability, capacity } and \textit{delay}.

\textbf{Connectivity} in a fog network can be improved by network relaying, partitioning and clustering. They can be used to reduce cost, trim data and expand connectivity. Connectivity is of course important, because if a device looses connection with the node, then it cannot offload work to it. 

\textbf{Reliability} is very difficult to provide in Fog computing. The methods used in traditional distributed systems like checkpointing and replication can be used. Since devices are more mobile and nodes dynamic, this can be hard to implement. It also implies that Fog nodes are connected to each other, which is not given in all architectures. 

For \textbf{capacity}, the Fog node need enough storage and good bandwidth. It does not matter how much storage it has, if it is slow to transfer it. To achieve good utilization of storage and bandwidth, it is important to analyse user patterns to know where data is most used. It’s difficult to know where to store data when it may need to be stored across several fog nodes. It is also hard to know when to send storage further to the distant cloud with bigger capacity at the cost of delay. 

Lastly, Fog focuses a lot on \textbf{delay}. It’s a good solution for “latency-sensitive” applications. The delay is reduced when using fog computing as the nodes are geographically closer to the client.







%----------------------------------








\section{Edge Computing}
Edge Computing is about utilizing processing power at the edge of the internet instead of using data centers. The edge of the internet is the devices that are closest to the users and clients. IoT devices generate a lot of data close to the users but reside far from the data centers. It is slow and inefficient to send the data to the distant cloud. Since IoT often requires real-time processing of data, it is better to move the processing to the edge, closer to the devices, to minimize the latency\cite{shi_edge_2016}.
A router, switch or Cloudlet are examples of edge devices. They can be utilized by the IoT devices to offload their computation to save time and energy.





%% ----------------------------------------------------------------------

\section{Ubiquitous Computing and Internet of Things}

Internet of Things(IoT) is a system of ubiquitous devices that are connected to the Internet\cite{noauthor_what_nodate}. Things with a wide array of sensors are placed everywhere to solve problems or give us information. An example of this is smartwatches. Smartwatches have several sensors to measure things like heartbeat, geographical location and speed, to name a few. These devices are often very small, have limited storage and have energy constraints because of limited battery capacity. They usually offload computation and storage via its connection to the internet, which is often a WiFi or cellular connection. 





%% ----------------------------------------------------------------------

\section{The Near-Far Computing Model}
The Near-Far Computing Model will be discussed further in a later chapter. However, a brief introduction is needed to understand what we are testing. The Near-Far Computing Model is a way to mitigate latency issues by having \textit{Near nodes} and \textit{Far nodes}. Near nodes are nodes that are geographically close to the client. This is beneficial, as the closer the clients are to the server, the lower the latency. Far nodes are powerful servers that typically reside in distant data centers. The Near node is closer but not as powerful as the Far node. This lets us create a balance where the Near node can do low response time work, while the Far node provides more resources for heavier computations and storage that can afford the higher latency.



%%---------------------------------------------------------------------

\section{The Slow Speed of Light}
The speed of light in a vacuum is 299,792 458 kilometers a second. This is the upper limit for how fast anything can travel. This also limits how fast electrons can move in cobber and how fast light can travel in fiber-optic cables. Signals in fiber-optic travel at about 2/3 of the speed of light. The speed of electrons in a copper wire is slightly faster than fiber. However, the clock speed of computers today is in the billions of times per second. A CPU running at 5GHz, in other words, 5 billion clock cycles a second, has time to do quite a lot while waiting for packets. When a packet has traveled just 1 foot, which takes one nanosecond, the CPU has already done five clock cycles. If the packet is traveling to another country, we can easily see that the CPU can do a significant amount of work while waiting for that packet. Since the speed of light is not going to change, the only way to improve latency is to reduce the physical distance between the nodes.




%% ----------------------------------------------------------------------------


\section{Why High Latency and Low Bandwidth Hurts}


Humans will easily detect delay and jitter when using applications\cite{satyanarayanan_case_2009}. Delay is almost impossible to fight when using hardware that is physically a long distance away. Jitter is impossible to prevent when using WAN. As a consequence, the user experience of applications using the WAN might suffer. Applications, where crispness and high response time is important, is called \textit{latency-aware applications}. Niraj Tolia et al.\cite{tolia_quantifying_2006} showed that we can divide user perceived responsiveness into several bins:
\begin{center}
\begin{tabular}{ | p{3cm} | p{5cm} | } 
    \hline
    Response Time& Perceived Responsiveness  \\ 
    \hline
    < 150ms & Crisp  \\ 
    150ms - 1s & Noticeable to annoying \\ 
    1s - 2s & Annoying \\ 
    2s - 5s & Unacceptable \\ 
    > 5s & Unusable \\ 
    \hline
\end{tabular}
\end{center}
The response time of applications is the sum of the time taken for packets to be sent between nodes, plus the processing time. Latency is not the only factor for how slow data transferring can be. Bandwidth is also a really important factor\cite{cerqueira_interactive_2007}. Some applications require very high bandwidth that is not necessarily available everywhere. When bandwidth is too low, the transferring time will take longer, affecting the response time.

Some applications require response time down to a few milliseconds. An example of this is when surgeons use a remote controlled robot to perform surgery. In this situation responsiveness need real-time to ensure safety. 







% ------------------------------------------------------------------------  

\section{Emerald}\label{Emerald}
Emerald is a programming language created in the period of 1984-1986 by Eric B. Jul, Norm Hutchinson, Hank Levy, and Andrew Black. It has several features that are important in distributed programming, like object mobility and type conformity. These features help to ease the development process of distributed systems. Emerald is an example of object orientation in distributed systems with similar performance as C++ in many aspects. This section will give an introduction to Emerald and are mainly based on the Emerald Language Report\cite{hutchinson_emerald_nodate}.

\subsection{Types and Conformity}
When programming distributed systems, each node has to agree on how to handle data. Emerald solves this with types and conformity. Each node does not need to transfer or replicate the whole object. It only needs to know what constants, variables and operations the object has available. These can be described with types. When invoking an object that resides on a remote node, it just needs to get a copy of the object type to know what to invoke and what to send. 

Everything is an object in Emerald. Even types are just first-class objects. 
Emerald is a compile-time strongly typed language. This means that the compiler will check that all objects conforms to the types at compile time, except for a few cases. An object conforms to a type if the following rules are followed:
\begin{enumerate}
    \item All operations defined in the type is available in the object
    \item For each operation, there are the same number of parameters, and each parameter conforms to the corresponding parameters.
\end{enumerate}
There are more checks, but these two are the most important ones.
Here is an example:
\begin{lstlisting}[language=emerald]
type TypeA
    Integer multiplyByTwo[n: Integer]
end TypeA

const ObjectA <- class ObjectA
    operation multiplyByTwo[n: Integer]
                        <- [res: Integer]
        res <- n * 2
    end multiplyByTwo
end ObjectA
\end{lstlisting}
Here, the ObjectA conforms to TypeA because we have the \textit{multiplyByTwo} operation. The type of the parameter and the type of the return is the same as described in the type. Having strong compile-time typing ensures that we always get the expected data, which reduces errors and failures.

To enable generic types and polymorphism, Emerald lets you check types during runtime with an operator: \verb|*>|. You can essentially read \verb|a *> b| as "\textit{a} conforms to \textit{b}". You also have \verb|view a as b| to help change the type of an object during runtime.




\subsubsection{Classes}
Emerald does not have classes, but provides the \verb|class| keyword as syntactic sugar for creating the concept of classes. Emerald will automatically create a type, a \textit{getSignature} operation, which returns the type, and finally a \textit{create} operation. These two objects are therefore semantically equivalent:
\begin{lstlisting}[language=emerald]
const A <- class A[x: Integer]
    export operation multiplyByTwo[n: Integer]
                        -> [res: Integer]
        res <- n * 2
    end multiplyByTwo
end A

const B <- object B
    const BType <- immutable typeobject BType
        operation multiplyByTwo[n: Integer]
                            -> [res: Integer]
    end BType
    export function getSignature -> [r: Signature]
        r <- BType
    end getSignature
    export operation create[x: Integer] -> [e: B]
        e <- immutable object aB
            export operation multiplyByTwo[n: Integer]
                        -> [res: Integer]
                res <- n * 2
            end multiplyByTwo
        end aB
    end create
end B
\end{lstlisting}
As you can see, using the \verb|class| keyword dramatically increases readability, gives us the concept of classes, and eases the development.

\subsection{Fields}\label{emerald:fields}
Emerald provides the keyword \verb|field| to automatically provide getters and setters.
\begin{lstlisting}[language=emerald]
const A <- class A
    field someVar : String <- ""
end A

const B <- class B
    var someVar: String <- ""
    
    export op getSomeVar -> [res: String]
        res <- someVar
    end getSomeVar
    
    export op setSomeVar[input: someVar]
        someVar <- input
    end setSomeVar
end B
\end{lstlisting}
In the above example, both class A and class B is equivalent. They will both have the operations \verb|getSomeVar| and \verb|setSomeVar| available.


\subsubsection{Immutability}
To ensure that objects do not change over time, we need \textit{immutability}. This is important in distributed systems, as changing an object that was not meant to change can break how it is supposed to interpret data, because we can have nodes with different versions of the object. Since Emerald only copies over the type to other nodes, it is important that these types are not able to change. In other words, the types should be immutable.  

Classes should also be immutable, if possible, to prevent confusion between nodes. The fewer mutable objects there are, the fewer errors and failures there can be.

\subsection{Object Mobility}
One of the key features of Emerald is Object Mobility. It has the tools needed to easily move objects around in a distributed system. Moving an object to a different node is as simple as this:
\begin{lstlisting}[language=emerald]
% x is created and resides on A
move x to B
% x now might reside on B
\end{lstlisting}
To visit all available nodes with your program is as simple as this\cite{noauthor_emerald_nodate}:
\begin{lstlisting}[language=emerald]
const Kilroy <- object Kilroy
  process
    const origin <-  locate self
    const up <- origin.getActiveNodes
    for e in up
      	const there <- e.getTheNode
      	move self to there
    end for
    move self to origin
  end process
end Kilroy
\end{lstlisting}
This short, compilable and runnable Emerald program will:
\begin{enumerate}
    \item Start a process.
    \item Find all active nodes.
    \item Loop over and visit all active nodes.
    \item Return to the original node.
\end{enumerate}


The move keyword is more of a hint to the Emerald VM that the object should be moved over. In Emerald you can use \verb|fix| to tell the VM that it has to move the object, instead of it just being a hint. It has the same syntax as move:
\begin{lstlisting}[language=emerald]
% x is created and resides on A
fix x at B
% x now resides on B, and will not move until unfixed.
\end{lstlisting}
To move the object again, we now have to unfix it, which is done by using \verb|unfix|:
\begin{lstlisting}[language=emerald]
unfix x
\end{lstlisting}
The object can now be moved or fixed at a different location. To make this procedure easier we have the \verb|refix| keyword. It will unfix and then fix an object to a new location. In other words these are equivalent:
\begin{lstlisting}[language=emerald]
unfix x
fix x at C
\end{lstlisting}
and
\begin{lstlisting}[language=emerald]
refix x at C
\end{lstlisting}
Therefore we use \verb|refix| almost everywhere when we are moving objects. 



\subsubsection{Attached}
When moving objects in Emerald, the VM is essentially just moving the pointer. This means that an object with some inner variables will stay on the original node when the parent is moved. If it makes sense to move the related data with the object, which it usually does, we can use the \verb|attached| keyword.
As shown in figure~\ref{fig:emerald_attached_figure}, when using the Attached keyword, both the object and the attached objects will move over when that parent object (x) is moved. If we do not move the related objects over, there will be a tremendous performance impact because local invocations are faster than remote invocations. Here is an example of how attached can be used:
\begin{lstlisting}[language=emerald]
const x <- class x
    attached const y <- someObject.create
    attached const z <- someObject.create
end x
\end{lstlisting}
In the above code example, one will get an object similar to the one shown in Node A in figure~\ref{fig:emerald_attached_figure} before the move.

\begin{figure}[t]
    \centering
    \textbf{Not attached vs Attached}\par\medskip
    \includegraphics[scale=0.8]{chapters/2_background/figures/emerald_attached.png}
    \caption{Showing a move executed without and with attached objects}
    \label{fig:emerald_attached_figure}
\end{figure}



\subsubsection{How the Emerald VM finds the location of objects}\label{cascading_search}
Emerald has a system for keeping track of and finding objects. To get the location of an object you can use the \verb|locate| keyword. For example:
\begin{lstlisting}[language=emerald]
const x <- 3
move x to B
var locationOfX : Node <- locate x
% The variable "locationOfX" now holds 
%  the Node object that resides on B.
\end{lstlisting}
Emerald uses something called \textit{Cascading Search} to locate an object. To prevent having to broadcast each time an object moves, a node only keeps track of the last position it knows the object resides or resided on. If the last known position of X is on A, then we ask A if he can send it. If A has later moved X to node C, then it will just send the query to C. This is called a \textit{forwarding reference}. It keeps track of the last known location (from its own perspective) and also a counter. Each time an object moves, that counter is incremented on the receiving node. If node A moves object X to B, then the counter is 0 at A and 1 at B. That means that the higher the counter, the more recent the pointer is. The counter is essentially replacing a global clock. The algorithm also has ways of dealing with failing nodes and lost pointers, but we will not go any deeper into that. 


\subsection{Remote Procedure Call}
Emerald uses \textit{Remote Procedure Calls}, which means that it will handle all remote invocations for the developer. Therefore, when programming, one can invoke the object as if it were local. This simplifies programming with distributed objects, as the developer do not need to think about programming each remote invocation. Here is a comparison:
\begin{lstlisting}[language=emerald, numbers=left]
const A <- class A
    export function multiplyByTwo[i: Integer] 
                                -> [res: Integer]
        res <- i * 2
    end mulitplyByTwo
end A

const B <- object B
    initially
        const x <- A.create
        % x is now a local pointer to A
        const result1 <- x.multiplyByTwo[2]
        
        refix x at someOtherNode
        % x is now a remote pointer to A
        const result2 <- x.multiplyByTwo[2] 
    end initially
end B
\end{lstlisting}
On line 12 and 16, we do the same invocation, but the location of the invoked function is different.

If we try to invoke a function on a node that is not available, Emerald will raise \verb|unavailable|.




\subsection{Failure and Unavailable}
When programming distributed systems we need to account for nodes going up or down. To help with this, Emerald has introduced \textit{unavailable handlers}. Unavailable handlers can be put at the bottom of process blocks, functions and compound blocks. Within an unavailable handler, one can try to recover by trying again or by trying other nodes.

\textbf{Failure Handlers} are invoked when a failure is raised by the program. An example of this is when invoking a nil object. The failure handler can be used to try to recover the program.

Below is an example of both failure and unavailable handlers:
\begin{lstlisting}[language=emerald, numbers=left]
const A <- class A
    export function divideTwoBy[i: Integer] 
                                -> [res: Integer]
        begin %compound statement
            res <- 2 / i
            failure
                % This will be executed if i is 0
            end failure
        end
    end mulitplyByTwo
end A

const B <- object B
    initially
        const x <- A.create
        % x is now a local pointer to A
        const result1 <- x.divideTwoBy[0]
        
        refix x at someOtherNode
        % x is now a remote pointer to A
        const result2 <- x.divideTwoBy[0]
        
        unavailable
            % this will be executed if someOtherNode
            %  is unavailable
        end unavailable
    end initially
end B
\end{lstlisting}






\subsection{Checkpoints and Recovery}
To help recovering from failure, Emerald provides checkpoints and recovery blocks. At any point, one can use the reserved \verb|checkpoint| keyword to create a checkpoint of an object. The checkpoint is a saved state on the machine so that the state is available on the disk instead of just RAM. If a node has restarted, then it will load the last checkpoint. When the program is recovering, the \verb|recovery| block is run instead of the \verb|initially| block. This is essential for creating more reliable distributed systems.



\subsection{Concurrency}
Emerald has support for processes. You use the process block like this:
\begin{lstlisting}[language=emerald]
const A <- class A
    process
        % do work here
    end process
end A
\end{lstlisting}

Emerald has also implemented monitors based on Hoare's monitors\cite{hoare_monitors_1974}. You can use the keywords \verb|monitor|, \verb|wait|, \verb|signal| and the \verb|Condition| type to easily have control over concurrency. Here is an example:
\begin{lstlisting}[language=emerald, numbers=left]
const A <- monitor class A
    const cond: Condition <- Condition.create
    export function multiplyByTwo[i: Integer] 
                                -> [res: Integer]
        wait cond % wait for your turn
        res <- i * 2
        signal cond % signal to _one_ other process
    end mulitplyByTwo
end A
\end{lstlisting}
In this example, line 6 will not be executed at the same time as other processes.







\subsection{Compiling and Running}
Emerald is compiled with the Emerald Compiler\cite{noauthor_emeraldold-emerald_2019}, which, when installed, can be run in a terminal with \verb|ec|. The code can be compiled with \verb|ec| like this:
\begin{lstlisting}[language=Bash]
ec file1.m file2.m 
\end{lstlisting}
To then run the program, you use the \verb|emx| which will run the compiled program on the Emerald VM:
\begin{lstlisting}[language=Bash]
emx file1.x file2.x
\end{lstlisting}

To start a remote node, run this on the remote node:
\begin{lstlisting}[language=Bash]
emx -U -R
\end{lstlisting}
Then all the other additional nodes is started like this:
\begin{lstlisting}[language=Bash]
emx -U -R172.17.0.2
\end{lstlisting}
Here \textbf{172.17.0.2} is the IP address of the first node.
Finally, to run the program run this:
\begin{lstlisting}[language=Bash]
emx -U -R172.17.0.2 file1.x file2.x
\end{lstlisting}
All the other nodes are now accessible, and can have objects moved to them.



%% ---------------------------------------------------------

\section{Docker}\label{background:docker}
Docker is a tool for running programs on the same architecture everywhere, without spinning up a whole virtual machine. It is built around the concept of \textit{containers}. Containers are small packages of software, that run on the Docker Engine. The Docker Engine is the the middleware between the containers and the operating system. This lets us run something locally and be able to expect the same behavior on other machines. Therefore, no matter where you run the application, it will always be the same environment from the perspective of the program. 

Because a Docker does not spin up a whole virtual machine, but rather share some resources between the containers \cite{dockercom_what_nodate}, it is more efficient. The container is essentially a sandbox, that runs on top of the OS. Each of the applications are therefore isolated, even though they have some shared resources.

To run Emerald we use a stripped debian image with i386 architecture, made by Oleks Shturmov\cite{oleks_oleksdocker-in5570v21_2021}.
In the referenced git repository, Oleks provides a makefile to easily set up Emerald.



%%------------------------------------------------------
\section{PlanetLab}
PlanetLab is a virtual organization consisting of many nodes spread out over the world. If an institution, like a university, wants to use PlanetLab, then they have to provide a PlanetLab server themselves. In other words, the institution gets sent a server, which they have to install and maintain to keep using PlanetLab. The result is a vast network that spans all over the world. At its peak, it had 1353 nodes spread across the earth\cite{noauthor_planetlab_nodate}. It is used as a wide-area network to research distributed systems. To get access you request for a DIKU, which is a collection of servers that you can log into. The login username for the servers is then the name of the DIKU.

Each of the nodes run some form of operating system. We can connect to them with Secure Shell(SSH). Each node we use in this thesis will have Docker available. To run Emerald on a PlanetLab server, transfer the Emerald source code or executable and the makefile for running Emerald, over to the server with \verb|scp|. For example, transfer the folder you are currently in to the home folder on the remote server:
\begin{lstlisting}[language=Bash]
scp -r . "planetlab-1.ida.liu.se":"~/"
\end{lstlisting}
Then ssh into the server: 
\begin{lstlisting}[language=Bash]
ssh -i ~/.ssh/planetlab \
    -l diku_name \
    "planetlab-1.ida.liu.se"
\end{lstlisting}
Here we provide a PlanetLab ssh key and the login name, which is the name of the DIKU, and a server URL.

When logged in on the server, we run the makefile with \verb|make| to start the shell within the Docker container. Then we can either run the executable or compile the code as described in the Emerald section \ref{Emerald}.



\section{Summary}
In this chapter, we have presented a theoretical foundation to help understand this thesis. We have described what a distributed system is, what it consists of, and some key features. We have given a short introduction to Edge, Fog, and Cloud computing. We have also made a short description of Near-Far computing, explained what speed of light has to do with latency, and showed how the Emerald eco-system works. Finally, we have described the environment the experiments will be conducted on, with Docker and PlanetLab.

