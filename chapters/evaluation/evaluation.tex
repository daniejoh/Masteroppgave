In this chapter we will evaluate the architectures described in chapter \ref{chapter:architectures} with the method described in chapter \ref{chapter:design_of_experiments}. Some of them will additionally be evaluated with the implementation from chapter \ref{chapter:implementation}.







\section{Bandwidth}
When evaluating we are making some assumptions about bandwidth. Table \ref{tab:Bandwidth_latency} shows latency and bandwidth for different technologies. The WiFi bandwidths are based on data from CenturyLink\cite{noauthor_24_nodate}. The 4G and 5G bandwidths are real-world examples from 4g.co.uk\cite{noauthor_how_nodate}. The 4G latency are from ping tests, while the 5G latency are from Verizon\cite{noauthor_what_2020}.

\begin{table}[h!]
    \centering
    \begin{tabular}[c]{|c|c|c|}
        \hline
        Technology & Latency (ms) & Bandwidth (Mbps) \\
            &   &  Download/Upload \\
        \hline
        \hline
        Wifi (2.4GHz) & <=1 & 150/150  \\
        \hline
        Wifi (5GHz) & <=1 & 450/450  \\
        \hline
        4G & 30-65 & 42/25  \\
        \hline
        5G & 30 & 200/100  \\
        \hline
        Wired LAN & <=1 & 1000+  \\
        \hline
        WAN & 10-300+ & 1000+  \\
        \hline
        
        
    \end{tabular}
    \caption{Latency and bandwidth for different technologies.}
    \label{tab:Bandwidth_latency}
\end{table}
When doing calculations in this chapter, we will be using numbers from table \ref{tab:Bandwidth_latency}.









\section{Multi-Access Edge Computing}
MEC seeks to offload a reasonable amount of work. Therefore, we will not offload all the work, but rather test with several ratios of computation offloading. It is not always efficient to offload all the work. According to Mach and Becvar´s survey\cite{mach_mobile_2017}, we can divide offloading into three categories:
\begin{itemize}
    \item \textit{Local execution}, where all the work is done on the mobile device.
    \item \textit{Full offloading}, where we offload all work to server(s).
    \item \textit{Partial offloading}, where we offload some of the work, and do some local.
\end{itemize}
We will compare these three. We will also test different variations of Partial offloading. According to cpubenchmark\cite{noauthor_passmark_nodate}, flagship phones is about half as fast as a standard desktop CPU. We assume that we are dealing with below average CPU's cellphones, as most people do not have the flagship phone. We will test different strengths of the MEC Server. We assume that the data centre will have plenty of resources. In total, we want to do 10000 iterations spread over all the nodes.

\subsection{Local execution}
%lacking geographic location?
\begin{table}[h!]
    \centering
    \begin{tabular}[c]{|c|c|c|c|}
        \hline
        Node type & Limitation & Iterations & Time used (s) \\
        \hline
        \hline
        Local & 50 & 10000 & 199.4 \\
        \hline
    \end{tabular}
    \caption{Only local execution}
    \label{tab:MEC_local_execution}
\end{table}
Table \ref{tab:MEC_local_execution} shows the time used for local execution only.



\subsection{Full execution}
%spread out over more nodes
Here we offload all the work to the nodes. Since the Far node has more resources we let it do more of the work.
\begin{table}[h!]
    \centering
    \begin{tabular}[c]{|c|c|c|c|}
        \hline
        Node type & Limitation & Iterations & Time used (s)\\
        \hline
        \hline
        Local & 50 & 0 & 0 \\
        \hline
        Near & 100 & 2500 & 23.3 \\
        \hline
        Far & 300 & 7500 & 30.5 \\
        \hline
    \end{tabular}
    \caption{Full offloading execution}
    \label{tab:MEC_full_execution}
\end{table}

\begin{table}[h!]
    \centering
    \begin{tabular}[c]{|c|c|c|c|}
        \hline
        Node type & Limitation & Iterations & Time used (s)\\
        \hline
        \hline
        Local & 50 & 0 & 0 \\
        \hline
        Near & 100 & 2800 & 27.3 \\
        \hline
        Far & 300 & 7200 & 29.8 \\
        \hline
    \end{tabular}
    \caption{Full offloading execution with more balance}
    \label{tab:MEC_full_execution_balanced}
\end{table}

Table \ref{tab:MEC_full_execution} and \ref{tab:MEC_full_execution_balanced} shows time used when offloading work to nodes. However, here we only send the workload once, and it does not need to do any communication until the work is done.

\begin{table}[h!]
    \centering
    \begin{tabular}[c]{|c|c|c|c|c|}
        \hline
        Node type & Limitation & Iterations & RTT to Local (ms)& Time used (s)\\
        \hline
        \hline
        Local & 50 & 0 & 0 & 0 \\
        \hline
        Near & 100 & 2800 & 30 & 94.8 \\
        \hline
        Far & 300 & 7200 & 76 & 588.0 \\
        \hline
    \end{tabular}
    \caption{Full offloading execution with interaction}
    \label{tab:MEC_full_execution_latency}
\end{table}

Table \ref{tab:MEC_full_execution_latency} shows how latency affects the work. The Near node had a latency 30ms RTT from the Local node. The Far node had a latency of 76ms RTT from the Local node.

\begin{table}[h!]
    \centering
    \begin{tabular}[c]{|c|c|c|c|c|}
        \hline
        Node type & Limitation & Iterations & RTT to Local (ms)& Time used (s)\\
        \hline
        \hline
        Local & 50 & 0 & 0 & 0 \\
        \hline
        Near & 100 & 7200 & 30 & 239.5 \\
        \hline
        Far & 300 & 2800 & 76 & 237.9 \\
        \hline
    \end{tabular}
    \caption{Full offloading execution with interaction with more balance in the nodes}
    \label{tab:MEC_full_execution_latency_balance}
\end{table}

Table \ref{tab:MEC_full_execution_latency_balance} shows a more balanced workload distribution. Both the near node and the far node used about the same time, which is optimal.

%TODO find out where they balance out. Finn en likning for det?
% planet7 og planet-70ms
% eksprementer med 5k totalt istedet for 10k
% Le


\begin{table}[h!]
    \centering
    \begin{tabular}[c]{|c|c|c|c|c|}
        \hline
        Node type & Number of & Limitation & Iterations & Time used (s)\\
         & HashWorkers & & & \\
        \hline
        \hline
        Local & 0 & 50 & 0 & 0 \\
        \hline
        Near & 2 & 100 & 2800 & 27.3 \\
        \hline
        Far & 5 & 300 & 7200 & 29.8 \\
        \hline
    \end{tabular}
    \caption{Full offloading execution with interaction}
    \label{tab:MEC_full_execution_latency_multiple}
\end{table}




\begin{figure}[t]
    \centering
    \includegraphics[scale=1]{chapters/evaluation/figures/times.png}
    \caption{Graph showing how interaction hurts. If x=10 then we get from Local every 10 iterations.}
    \label{fig:time_graph_near_far}
\end{figure}

Figure \ref{fig:time_graph_near_far} shows how latency affects the time used when you have to constantly get info from the mobile device (Local). If you constantly have to get info from the mobile device, the time to complete the whole task will be much slower. If you don't need to get info that often, then the Far server will yield better results as it has more computational power.





\subsection{Partial execution}
For partial execution, we want all nodes to work.
\begin{table}[h!]
    \centering
    \begin{tabular}[c]{|c|c|c|c|}
        \hline
        Node type & Limitation & Iterations & Time used (s)\\
        \hline
        \hline
        Local & 50 & 1000 & 19.2 \\
        \hline
        Near & 100 & 2000 & 19.3 \\
        \hline
        Far & 300 & 7000 & 29.0 \\
        \hline
    \end{tabular}
    \caption{Partial offloading execution}
    \label{tab:MEC_partial_execution}
\end{table}


\begin{table}[h!]
    \centering
    \begin{tabular}[c]{|c|c|c|c|}
        \hline
        Node type & Limitation & Iterations & Time used (s)\\
        \hline
        \hline
        Local & 50 & 1200 & 23.2 \\
        \hline
        Near & 100 & 2600 & 25.3 \\
        \hline
        Far & 300 & 6200 & 25.5 \\
        \hline
    \end{tabular}
    \caption{Partial offloading execution with more balance}
    \label{tab:MEC_partial_execution_balanced}
\end{table}
Table \ref{tab:MEC_partial_execution} and \ref{tab:MEC_partial_execution_balanced} shows offloading with little overhead and no latency.






\subsection{Comparison}
As we can see from table \ref{tab:MEC_partial_execution1} and \ref{tab:MEC_partial_execution2}, there has to be a balance of how much work is offloaded to get the fastest result. In this thesis we will not focus on offloading algorithms, but they are researched and exists. If we look at table \ref{tab:MEC_local_execution}, we can see that we get significant speedup by offloading.  %Discussion?

%TODO
% Gjøre målinger med overføring av filer først. Finn data på bandwidth og pluss det på tiden.
% Gjøre målinger hvor det kreves mer samhandling mellom nodene. Vi må se latency!


%\cite{mach_mobile_2017} for hvor mye som skal offloades.!!!!
% test med 100% offload, 50% offload osv
%\begin{itemize}
 %   \item Easily scalable as we have the common interface. This makes it easy to add more vms to run more apps. So, its horizontally scalable?
%\end{itemize}


% \subsection{Computation}% move this over to evaluation?
% In MCC most of the computation will be done on the closest server. This is because MCC aims to have as little as possible computation on the mobile device to conserve computation, storage and most importantly, energy.

% \subsection{storage}% move this over to evaluation?
% MCC will have little storage offloading. The device itself might have little storage, but in todays society most of them have enough local storage for what they need to do. However, in some cases there might be need for storage offloading.







\section{Cloudlets}







\section{Amazon Cloudfront}
TODO: Warm up time for lambda.
\section{Summary}
TODO

\section{Comparison of the architectures}
TODO