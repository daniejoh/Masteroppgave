\chapter{Architectures}\label{chapter:architectures} 
In this chapter, we have researched and will present what already exists of some architectures that relate to The Near-Far Computing Model. We will give an overview of each architecture.




\section{Multi-Access Edge Computing}\label{section:MEC_architecture}
Multi-Access Edge Computing(MEC), also known as Mobile Edge Computing, was originally an architecture that proposes to utilize the cellular network for having a close-by MEC server\cite{porambage_survey_2018}. A MEC server is a server that has software and hardware designed to handle work and storage offloading. If needed, a distant data center or CDN can aid the MEC server with even more storage and computation.
\begin{figure}[t]
    \centering
    %\textbf{Mobile Edge Computing}\par\medskip
    \includegraphics[scale=0.75]{chapters/4_architectures/figures/MEC.png}
    \caption{Diagram of MEC}
    \label{fig:MEC}
\end{figure}

\subsection{Offloading}
Figure \ref{fig:MEC} shows how MEC works on a high level. MEC will utilize the ubiquitous cellular network to offload its work. There will be a MEC server at the base station that can provide storage and computational power to the mobile devices. Since the base station is just one hop away, we do not have to deal with the dynamic and unreliable WAN. Additionally, the cellular network provides a very small delay, as base stations are relatively close to almost everywhere where there are developed civilizations. If the servers at the base station are overloaded, they can send the work further to the data centers of the distant cloud or a CDN. Mobile devices can be any device that needs storage offloading. The only requirement for this architecture is that it has a cellular or WiFi connection.

The MEC server architecture is divided into several layers that together form three systems\cite{patel_mec_nodate}. At the bottom is the hardware layer. Over that is the virtualization layer. These two layers are part of the MEC Hosting Infrastructure Management System. Over that is the MEC Application Platform Management System which consists of a MEC virtualization manager layer and MEC Application Platform Services layer. The virtualization manager can provide IaaS. The MEC Application Platform Services layer is an interface for the virtual machines that the applications are running on. The VMs and the apps form the Application Management Systems. Figure \ref{fig:MEC_Server} is a simplified version of the server architecture presented in \cite{patel_mec_nodate}. It shows that we can have several applications running simultaneously on the MEC Server. All the Applications run in a VM that the MEC Application Platform Services control.
\begin{figure}[t]
    \centering
    \includegraphics[scale=0.3]{chapters/4_architectures/figures/MEC_server_screenshot.png}
    \caption{Simplified version of the MEC Server architecture}
    \label{fig:MEC_Server}
\end{figure}

%TODO? snakke om NFV og SDN
\subsection{Network Functions Virtualization}
Network Functions Virtualization(NFV) is used in many MEC implementations\cite{patel_mec_nodate}. Networks need to run DNS, Firewall, NAT, et cetera to run a modern network. Traditionally all of these functions have their own hardware. However, this makes infrastructure expensive and hard to scale. NFV was introduced as a way to virtualize these functions. These functions then run on programmable switches and servers. Since they are easily controlled, they are more scalable and flexible.

\subsection{Software-Defined Networking}
Software-Defined Networking(SDN) is often used together with NFV. Instead of limiting the data layer to the hardware of switches, we run a VM on some hardware that lets networking be programmable. We can then set up rules and define where each packet should go. Since the network is now easily programmable, we can let the programmers control how the network is configured. The programmers can then change the configuration on the fly and adapt the network to the context needed.



% ------------------------------------------------------------------------------------------

\section{Cloudlet} \label{section:architecutres_cloudlet} 
Satyanarayanan et al., proposed in their paper “The Case for VM-Based Cloudlets in Mobile Computing”\cite{satyanarayanan_case_2009} a solution to resource poverty of mobile devices and for the high latency in the Wide-Area Network(WAN). They wanted something that could fulfill the need for real-time responsiveness by having low-latency, one-hop, high-bandwidth access to a nearby \textit{Cloudlet}. They describe Cloudlets as a "data center in a box"\cite{satyanarayanan_case_2009}. They are small computers that will run locally at each location where they are needed. All it needs is access to the internet and power. An example is setting it up in a coffee shop. There it will be able to serve customers with computation and storage if needed. All the coffee shop needs to do is plug it in and configure the Cloudlet. Since it is a relatively small box, it will not take much space or be in the way of customers or employees. Having the Cloudlet so close will help the response time stay low and be predictable.

In this architecture, they focus on having as much as possible of the computation done on the nearby Cloudlet. The mobile device is just a thin client and offloads all computation to the Cloudlet. If no Cloudlet is available, then the mobile device will try to offload to the distant cloud or use its own computing power if all else fails. This will heavily affect performance, especially if it has to do the work locally. However, if it detects a nearby Cloudlet, it can easily migrate the work over to it and return to full functionality.
 
\begin{figure}[t]
    \centering
    \includegraphics[scale=0.9]{chapters/4_architectures/figures/Cloudlet.png}
    \caption{Illustration of a simple Cloudlet connection}
    \label{fig:Cloudlet}
\end{figure}


Figure \ref{fig:Cloudlet} shows a simple overview of how Cloudlets are connected. A mobile device will use WiFi or cellular to connect to its closest access point. The access point is further connected to one or more Cloudlets with a LAN connection. Since they are so close in proximity to the mobile device and on LAN, it can easily achieve just 1ms round-trip-time to the Cloudlet. In their paper, Satyanarayanan et al.\cite{satyanarayanan_case_2009} also propose that that ideally, the Cloudlets themselves can be the access point. This way, the Cloudlet is truly one-hop away. The mobile device is usually connected to just one Cloudlet but can easily be connected to several if needed. The Cloudlets are further connected to the distant Cloud. Not shown in the figure is the case when there are no Cloudlets available. It will then connect directly to the Cloud instead of via a Cloudlet. 


\subsection{Offloading}
To simplify deployment, Cloudlets use \textit{transient customization of Cloudlet infrastructure}. They want to use VMs that are fast and easy to deploy on the Cloudlets. Each time a mobile device wants to use a Cloudlet, it will send a small overhead to the server that contains enough information to start a VM that can be used to offload the work. By using VMs they ensure that creating programs for the Cloudlet is not limited to a small API or programming language. In other words, they let the developers decide the environment. The Cloudlet will get rid of the VM after it is finished with its use. This ensures that the Cloudlet is in the same state as before it was used.



% ------------------------------------------------------------------------------------------



\section{Google Anthos}
Google Cloud Platform(GCP) has been providing Cloud solutions since 2008. Like many other cloud providers, a client can rent hosted servers on GCP. In 2019 Google announced Google Anthos, also known as just Anthos. Anthos is a way to bring GCP to the edge and on-premises\cite{noauthor_anthos_nodate}. A client can easily install Anthos software on one of their on-premises machines and connect it to their GCP. In the GCP console, the client can then control and configure this new node in the Anthos control panel. You can then set up a cluster consisting of nodes that resides on the edge and in the data center, and let them easily collaborate with each other. GCP will control the device that the client has connected to Anthos. This ensures that they do not have to think about security.

\subsection{Offloading}
Google Anthos uses Kubernetes\cite{noauthor_production-grade_nodate} to easily create distributed systems\cite{noauthor_anthos_nodate}. This means that everything that runs on the Anthos platform uses container software and middleware like Docker(see section \ref{background:docker}). Every program that runs on it is within a Docker container. Due to containerization of programs, we can easily distribute them on this platform. This enables us to easily offload work to edge nodes and nodes in the distant data center.
\begin{figure}[t]
    \centering
    \includegraphics[scale=0.9]{chapters/4_architectures/figures/anthos_architecture.png}
    \caption{Illustration of a simple two nodes running on Anthos}
    \label{fig:Anthos_architecture}
\end{figure}
Figure \ref{fig:Anthos_architecture} show a simplified overview of two nodes running on Anthos. Anthos middleware runs in each node and controls a Kubernetes cluster. This Kubernetes cluster are then able to deploy containers where needed.
% Uses devices in your home like the chromecast or google home?
%Gir deg muligheten til å deploye on the edge, "on-premises". Veldig lik de andre
%https://cloud.google.com/anthos/docs/concepts/overview
%kanskje bare sammenligne characteristics for denne? På et vis, bare droppe å bruke programmet fordi det uansett blir så likt.


% -------------------------------------------------------------------------------

\section{Amazon Cloudfront}
Amazon Web Services (AWS) provides Amazon Cloudfront as a solution for latency-aware applications\cite{noauthor_production-grade_nodate}. Amazon Cloudfront is a CDN service that lets a client integrate other AWS services at the edge. This lets the client easily provide storage and computation at the edge. Additionally, they get the safety that is already integrated in the services that AWS provides. 

\subsection{Offloading}
Within Amazon Cloudfront, they provide a service called Lambda@Edge. AWS Lambda is a serverless service that lets a client run small functions in the cloud so that they do not need a whole server for a small piece of code. Also, it lets them easily scale if need be. Lambda@Edge enables putting these Lambda functions on the edge, and therefore closer to the user. It then lets clients easily connect to their database services to provide storage. This storage can also be at the edge if needed. Additionally, one can rent servers at the edge. This requires more setup but gives more control.



% ------------------------------------------------------------------------------

\section{Akamai}
Akamai is a company that specializes in bringing content closer to the edge by using Content Delivery Networks(CDN). It has become widely popular, and a significant portion of the web moves through an Akamai device. They will have servers directly at the ISPs, so clients are just one "network hop" away from their servers. Akamai\cite{noauthor_exceptional_nodate} claims \say{85\% of the world's Internet users are within a single "network hop" of an Akamai edge server.}. They also claim to handle 15-30\% of the world's web traffic. Companies pay Akamai to cache videos and web pages closer to the user. This helps improve latency and stability, as interaction through Wide-Area Network is minimized. 

\begin{figure}[t]
    \centering
    \includegraphics[scale=0.8]{chapters/4_architectures/figures/akamai_dist.png}
    \caption{Simplified Illustration of replication in Akamai CDN}
    \label{fig:Akamai_cdn}
\end{figure}

Figure \ref{fig:Akamai_cdn} shows how content is replicated across the world. A big server rack might be in Washington and contain the web content for a page for a local university. If we were to get this page from the other side of the world in Norway, then there will be significant latency and instability when trying to retrieve the content. The server will therefore benefit from replicating the page to an Akamai server in Oslo, Norway. Then when someone in Norway tries to retrieve the page, it will instead get it from the Edge server that is geographically close to the end-user. However, should one of the edge servers be overloaded or fail, then it will let clients get content from the original server in Washington. 




\section{Summary}
In this chapter, we have presented several architectures that focus on mitigating latency. We have presented how they work and how they are set up. We have focused on presenting how they offload work or storage where applicable. 